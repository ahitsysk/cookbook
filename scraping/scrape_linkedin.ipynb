{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping LinkedIn with MultiOn\n",
    "\n",
    "In this recipe, we'll demonstrate how to scrape social network data from LinkedIn using MultiOn's Agent API in local mode. We'll be using `step` and `retrieve` to scrape all profiles relevant to MultiOn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up the environment\n",
    "\n",
    "First, let's install the required libraries and set up the MultiOn client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install multion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multion.client import MultiOn\n",
    "\n",
    "client = MultiOn(\n",
    "    api_key=\"YOUR_API_KEY\" # Get your API key from https://app.multion.ai/api-keys\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the search agent\n",
    "\n",
    "Next, we will create an agent session with local mode enabled, which allows us to see the agent in action in our local browser with the MultiOn Chrome Extension. This agent will be responsible for searching and navigating company search results.\n",
    "\n",
    "Make sure that the MultiOn Chrome Extension is installed and enabled (for more details, see [here](https://docs.multion.ai/learn/browser-extension))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_response = client.sessions.create(\n",
    "    url=\"https://linkedin.com\",\n",
    "    local=True\n",
    ")\n",
    "\n",
    "session_id = create_response.session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't logged in to LinkedIn yet, make sure you do so. Now, we can continue from the same session using the `session_id` from the response. We will search for people related to MultiOn and use `step` until we get to the people results page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"CONTINUE\"\n",
    "\n",
    "while status == \"CONTINUE\":\n",
    "    step_response = client.sessions.step(\n",
    "        session_id=session_id,\n",
    "        cmd=\"Search for MultiOn and see all people results.\"\n",
    "    )\n",
    "    status = step_response.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Scrape search results\n",
    "\n",
    "Once we are on the results page, we can start retrieving data with `retrieve`.\n",
    "\n",
    "Since LinkedIn paginates its results, we will have to navigate to the next page with `step` once we have scraped all profiles on the current page. Conveniently, `retrieve` has an option to scroll down to the bottom while retrieving (`scroll_to_bottom`), which we will use to speed up the process. We can also enable `render_js` to get all image URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_profiles = []\n",
    "has_more = True\n",
    "page = 1\n",
    "\n",
    "while has_more:\n",
    "    retrieve_response = client.retrieve(\n",
    "        session_id=session_id,\n",
    "        cmd=\"Get all profiles with name, headline, location, current position, profile URL, and image URL.\",\n",
    "        fields=[\"name\", \"headline\", \"location\", \"current_position\", \"profile_url\", \"image_url\"],\n",
    "        scroll_to_bottom=True,\n",
    "        render_js=True\n",
    "    )\n",
    "    scraped_profiles.extend(retrieve_response.data)\n",
    "    print(f\"Scraped page {page} with {len(retrieve_response.data)} profiles\")\n",
    "    page += 1\n",
    "    step_response = client.sessions.step(\n",
    "        session_id=session_id,\n",
    "        cmd=\"Click the 'Next' button to go to the next page.\"\n",
    "    )\n",
    "    has_more = \"last page\" not in step_response.message\n",
    "\n",
    "print(f\"Scraped {len(scraped_profiles)} profiles:\\n{scraped_profiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Scrape profiles in parallel\n",
    "\n",
    "If we want to get all the details from each profile, we will have to call `retrieve` on each individual profile page. To speed up the process, we can use `retrieve` in parallel for each URL we previously scraped from the search results. Note that we will be using remote mode by calling `retrieve` without `session_id` or `local`, since you probably don't want more than 10 browser windows open concurrently on your computer.\n",
    "\n",
    "⚠️ We will be calling `retrieve` many times at once, beware of costs and rate limits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def fetch_profile_details(profile):\n",
    "    profile_details_response = client.retrieve(\n",
    "        url=profile[\"profile_url\"],\n",
    "        cmd=\"Get professional details with about, experience as array, education as array, skills as array, honors as array, and languages as array.\",\n",
    "        fields=[\"about\", \"experience\", \"education\", \"skills\", \"honors\", \"languages\"]\n",
    "    )\n",
    "    profile[\"details\"] = profile_details_response.data\n",
    "    return profile\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    scraped_profiles = list(executor.map(fetch_profile_details, scraped_profiles))\n",
    "\n",
    "print(f\"Scraped detailed profiles:\\n{scraped_profiles}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
